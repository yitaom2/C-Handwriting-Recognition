{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Convolution2D, Dense, Flatten, MaxPooling2D\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)\n",
    "x_test= x_test.reshape(10000, 28, 28, 1)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "for i in range(10000):\n",
    "    a = \"image/x_test/\" + str(i) + \".bmp\"\n",
    "    b = \"image/y_test/\" + str(i)\n",
    "#     np.savetxt(a, x_test[i,:,:,0])\n",
    "    np.savetxt(b, y_test[i])\n",
    "    cv2.imwrite(a, x_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 01:23:05.062900 140612260312896 deprecation_wrapper.py:119] From /home/yitao/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0723 01:23:05.094963 140612260312896 deprecation_wrapper.py:119] From /home/yitao/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0723 01:23:05.098459 140612260312896 deprecation_wrapper.py:119] From /home/yitao/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0723 01:23:05.125525 140612260312896 deprecation_wrapper.py:119] From /home/yitao/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0723 01:23:05.226703 140612260312896 deprecation_wrapper.py:119] From /home/yitao/.local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0723 01:23:05.235894 140612260312896 deprecation_wrapper.py:119] From /home/yitao/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(96, (3, 3), padding=\"SAME\", input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2),padding='SAME'))\n",
    "model.add(Convolution2D(256, (3, 3), padding=\"SAME\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=20, activation=\"relu\"))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 01:23:06.096253 140612260312896 deprecation.py:323] From /home/yitao/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0723 01:23:06.165216 140612260312896 deprecation_wrapper.py:119] From /home/yitao/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 212s 4ms/step - loss: 1.0985 - acc: 0.7066\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 195s 3ms/step - loss: 0.5057 - acc: 0.8846\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 181s 3ms/step - loss: 0.2439 - acc: 0.9357\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 177s 3ms/step - loss: 0.1510 - acc: 0.9647\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 177s 3ms/step - loss: 0.1216 - acc: 0.9697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe2861c6f98>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=100, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 749us/step\n",
      "[0.15182011344382773, 0.9671000051498413]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(x_test, y_test, batch_size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/weights.h5') #保存权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 1, 96) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_1/bias:0' shape=(96,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 96, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_2/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(50176, 20) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(20,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_2/kernel:0' shape=(20, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     weights = layer.get_weights()\n",
    "#     if weights == []:\n",
    "#         continue\n",
    "        \n",
    "#     name = layer.get_config()['name']\n",
    "#     print(name)\n",
    "#     directory_name = \"layerData/\" + name \n",
    "#     if not os.path.isdir(directory_name):\n",
    "#         os.makedirs(directory_name)\n",
    "        \n",
    "#     if name.find('conv2d') != -1:\n",
    "#         layerNum = np.shape(layer.get_weights()[0])[3]\n",
    "#         imageNum = np.shape(layer.get_weights()[0])[2]\n",
    "        \n",
    "#         np.savetxt(directory_name + \"/bias\", weights[1])\n",
    "#         for j in range(imageNum):\n",
    "#             for i in range(layerNum):\n",
    "#                 sub_dir = directory_name + \"/imageNum\" + str(j)\n",
    "#                 if not os.path.isdir(sub_dir):\n",
    "#                     os.makedirs(sub_dir)\n",
    "#                 np.savetxt(sub_dir + \"/\" + str(i), weights[0][:,:,j,i])\n",
    "#     elif name.find('dense') != -1:\n",
    "#         np.savetxt(directory_name + \"/weight\", weights[0])\n",
    "#         np.savetxt(directory_name + \"/bias\", weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"layers.txt\", \"w\")\n",
    "for layer in model.layers:\n",
    "    temp = layer.get_config()\n",
    "    name = temp['name']\n",
    "    f.write(name)\n",
    "    weights = layer.get_weights()\n",
    "    \n",
    "    if name.find('flatten') != -1:\n",
    "        f.write(\" \" + str(layer.get_input_shape_at(0)[3]) + \",\" \n",
    "                + str(layer.get_input_shape_at(0)[1]) + \",\" \n",
    "                + str(layer.get_input_shape_at(0)[2])) #LayDim\n",
    "        f.write(\"\\n\")\n",
    "        continue\n",
    "        \n",
    "    if name.find('max_pooling2d') != -1:\n",
    "        f.write(\" \" + str(layer.get_input_shape_at(0)[3]) + \",\" \n",
    "                + str(layer.get_input_shape_at(0)[1]) + \",\" \n",
    "                + str(layer.get_input_shape_at(0)[2])) #LayDim\n",
    "        kernel_size = temp['pool_size']\n",
    "        f.write(\" \" + str(kernel_size[0]) + \",\" + str(kernel_size[1])) #maskDim\n",
    "        f.write(\" \" + str(temp['strides'][0])) #stride\n",
    "        f.write(\" \" + temp['padding'].upper()) #ZeroPadding\n",
    "        f.write(\"\\n\")\n",
    "        continue\n",
    "        \n",
    "    directory_name = \"layerData/\" + name \n",
    "    if not os.path.isdir(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "        \n",
    "    if name.find('conv2d') != -1:\n",
    "        f.write(\" \" + str(layer.get_input_shape_at(0)[3]) + \",\" \n",
    "                + str(layer.get_input_shape_at(0)[1]) + \",\" \n",
    "                + str(layer.get_input_shape_at(0)[2])) #LayDim\n",
    "        kernel_size = temp['kernel_size']\n",
    "        f.write(\" \" + str(temp['filters']) + \",\" + str(kernel_size[0]) + \",\" + str(kernel_size[1])) #maskDim\n",
    "        f.write(\" \" + str(temp['strides'][0])) #stride\n",
    "        f.write(\" \" + temp['padding'].upper()) #ZeroPadding\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        layerNum = np.shape(layer.get_weights()[0])[3]\n",
    "        imageNum = np.shape(layer.get_weights()[0])[2]\n",
    "        \n",
    "        np.savetxt(directory_name + \"/bias\", weights[1])\n",
    "        for j in range(imageNum):\n",
    "            for i in range(layerNum):\n",
    "                sub_dir = directory_name + \"/imageNum\" + str(j)\n",
    "                if not os.path.isdir(sub_dir):\n",
    "                    os.makedirs(sub_dir)\n",
    "                np.savetxt(sub_dir + \"/\" + str(i), weights[0][:,:,j,i])\n",
    "    elif name.find('dense') != -1:\n",
    "        f.write(\" \" + str(layer.get_input_shape_at(0)[1]))\n",
    "        f.write(\" \" + str(layer.get_output_shape_at(0)[1]))\n",
    "        f.write(\" \" + str(layer.get_config()['activation']))\n",
    "        f.write(\"\\n\")\n",
    "        np.savetxt(directory_name + \"/weight\", weights[0])\n",
    "        np.savetxt(directory_name + \"/bias\", weights[1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0136194  -0.00958493 -0.00073742 -0.00266215 -0.00068504  0.00390912\n",
      "  0.00325519  0.0164456   0.00937561  0.01428937 -0.01288039 -0.00343537\n",
      "  0.00707347 -0.00858768  0.00404946 -0.00830341]\n",
      "[ 0.00363804  0.01113191  0.01497999  0.00427318 -0.00335229  0.0042916\n",
      "  0.01235507  0.00294866 -0.00664267 -0.01510338 -0.01272875  0.0028398\n",
      "  0.00300649 -0.01331764  0.00380878  0.00629887]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"layers.txt\", \"w\")\n",
    "for layer in model.layers:\n",
    "    temp = layer.get_config()\n",
    "    name = temp['name']\n",
    "    f.write(name)\n",
    "    weights = layer.get_weights()\n",
    "        \n",
    "    if name.find('conv2d') != -1:\n",
    "        print( weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "# 输入数据  \n",
    "import input_data  \n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)  \n",
    "# 定义网络超参数  \n",
    "learning_rate = 0.001  \n",
    "training_iters = 200000  \n",
    "batch_size = 64  \n",
    "display_step = 20  \n",
    "# 定义网络参数  \n",
    "n_input = 784  # 输入的维度  \n",
    "n_classes = 10 # 标签的维度  \n",
    "dropout = 0.8  # Dropout 的概率  \n",
    "# 占位符输入  \n",
    "x = tf.placeholder(tf.types.float32, [None, n_input])  \n",
    "y = tf.placeholder(tf.types.float32, [None, n_classes])  \n",
    "keep_prob = tf.placeholder(tf.types.float32)  \n",
    "# 卷积操作  \n",
    "def conv2d(name, l_input, w, b):  \n",
    "    return tf.nn.relu(tf.nn.bias_add( \\  \n",
    "    tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding='SAME'),b) \\  \n",
    "    , name=name)  \n",
    "# 最大下采样操作  \n",
    "def max_pool(name, l_input, k):  \n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], \\  \n",
    "    strides=[1, k, k, 1], padding='SAME', name=name)  \n",
    "# 归一化操作  \n",
    "def norm(name, l_input, lsize=4):  \n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)  \n",
    "# 定义整个网络   \n",
    "def alex_net(_X, _weights, _biases, _dropout):  \n",
    "    _X = tf.reshape(_X, shape=[-1, 28, 28, 1]) # 向量转为矩阵  \n",
    "    # 卷积层  \n",
    "    conv1 = conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])  \n",
    "    # 下采样层  \n",
    "    pool1 = max_pool('pool1', conv1, k=2)  \n",
    "    # 归一化层  \n",
    "    norm1 = norm('norm1', pool1, lsize=4)  \n",
    "    # Dropout  \n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)  \n",
    "   \n",
    "    # 卷积  \n",
    "    conv2 = conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])  \n",
    "    # 下采样  \n",
    "    pool2 = max_pool('pool2', conv2, k=2)  \n",
    "    # 归一化  \n",
    "    norm2 = norm('norm2', pool2, lsize=4)  \n",
    "    # Dropout  \n",
    "    norm2 = tf.nn.dropout(norm2, _dropout)  \n",
    "   \n",
    "    # 卷积  \n",
    "    conv3 = conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])  \n",
    "    # 下采样  \n",
    "    pool3 = max_pool('pool3', conv3, k=2)  \n",
    "    # 归一化  \n",
    "    norm3 = norm('norm3', pool3, lsize=4)  \n",
    "    # Dropout  \n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)  \n",
    "   \n",
    "    # 全连接层，先把特征图转为向量  \n",
    "    dense1 = tf.reshape(norm3, [-1, _weights['wd1'].get_shape().as_list()[0]])   \n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1')   \n",
    "    # 全连接层  \n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') \n",
    "    # Relu activation  \n",
    "    # 网络输出层  \n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']  \n",
    "    return out  \n",
    "   \n",
    "# 存储所有的网络参数  \n",
    "weights = {  \n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64])),  \n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),  \n",
    "    'wc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),  \n",
    "    'wd1': tf.Variable(tf.random_normal([4*4*256, 1024])),  \n",
    "    'wd2': tf.Variable(tf.random_normal([1024, 1024])),  \n",
    "    'out': tf.Variable(tf.random_normal([1024, 10]))  \n",
    "}  \n",
    "biases = {  \n",
    "    'bc1': tf.Variable(tf.random_normal([64])),  \n",
    "    'bc2': tf.Variable(tf.random_normal([128])),  \n",
    "    'bc3': tf.Variable(tf.random_normal([256])),  \n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),  \n",
    "    'bd2': tf.Variable(tf.random_normal([1024])),  \n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))  \n",
    "}  \n",
    "# 构建模型  \n",
    "pred = alex_net(x, weights, biases, keep_prob)  \n",
    "# 定义损失函数和学习步骤  \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))  \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)  \n",
    "# 测试网络  \n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))  \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))  \n",
    "# 初始化所有的共享变量  \n",
    "init = tf.initialize_all_variables()  \n",
    "# 开启一个训练  \n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)  \n",
    "    step = 1  \n",
    "    # Keep training until reach max iterations  \n",
    "    while step * batch_size < training_iters:  \n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)  \n",
    "        # 获取批数据  \n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})  \n",
    "        if step % display_step == 0:  \n",
    "            # 计算精度  \n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})  \n",
    "            # 计算损失值  \n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})  \n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)  \n",
    "        step += 1  \n",
    "    print \"Optimization Finished!\"  \n",
    "    # 计算测试精度  \n",
    "    print \"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.})  \n",
    " \n",
    "以上代码忽略了部分卷积层，全连接层使用了特定的权重。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
